{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPaRPwSvsUI6I3eTI54JJJg"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["from google.colab import files\n","\n","# Upload the file from your local machine\n","uploaded = files.upload()"],"metadata":{"id":"H2pYOj-1YrV0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Ques 1"],"metadata":{"id":"WJYfnE7DA9EL"}},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.model_selection import KFold, train_test_split\n","from sklearn.metrics import r2_score\n","import matplotlib.pyplot as plt\n","\n","dataset = pd.read_csv('USA_Housing (1).csv')\n","\n","X = dataset.drop('Price', axis=1)\n","y = dataset['Price']\n","\n","print(\"Input features shape:\", X.shape)\n","print(\"Output variable shape:\", y.shape)\n","\n","\n","\n","#scale, cleaning etc, then feature extr or corr, heatmap\n","\n","\n","\n","#-----------Data transformation- scaling/normalization (mean method)\n","# Initialize the StandardScaler\n","scaler = StandardScaler()\n","# Fit and transform the input features\n","X_scaled = scaler.fit_transform(X)\n","# Convert the scaled features back to a DataFrame for convenience\n","X_scaled = pd.DataFrame(X_scaled, columns=X.columns)\n","print(\"Scaled features sample:\")\n","print(X_scaled.head())\n","\n","\n","#---------------cleaning- missing values\n","from sklearn.impute import SimpleImputer\n","median_imputer = SimpleImputer(missing_values=np.nan, strategy=\"median\")\n","# Fit the imputer on the training data\n","median_imputer.fit(X_scaled)\n","# Transform the training data\n","X_scaled = median_imputer.transform(X_scaled)\n","\n","\n","\n","# Step 2: Cleaning the data (using z-score to remove outliers)\n","z_scores = np.abs(stats.zscore(X_scaled))\n","threshold = 3  # Defining the threshold for outliers\n","X_scaled_df = X_scaled[(z_scores < threshold).all(axis=1)]  # Keep rows with z-scores less than the threshold\n","\n","# Ensure that shape of X_scaled_df is correct after filtering\n","#print(\"Shape after z-score filtering:\", X_scaled_df.shape)\n","X_scaled = pd.DataFrame(X_scaled, columns=X.columns)\n","# Step 3: Extreme value analysis (using IQR to remove outliers)\n","Q1 = X_scaled.quantile(0.25)\n","Q3 = X_scaled.quantile(0.75)\n","IQR = Q3 - Q1\n","\n","# Define lower and upper bounds\n","lower_bound = Q1 - 1.5 * IQR\n","upper_bound = Q3 + 1.5 * IQR\n","\n","# Step 4: Remove outliers based on IQR\n","filtered_data = X_scaled[(X_scaled >= lower_bound) & (X_scaled <= upper_bound)].dropna()\n","\n","# Ensure that the number of columns is consistent with the original dataset\n","print(\"Filtered data shape after outlier removal:\", filtered_data.shape)\n","\n","# Initialize KFold with 5 splits\n","kf = KFold(n_splits=5, shuffle=True, random_state=42)\n","# Store R2 scores and beta coefficients for each fold\n","r2_scores = []\n","beta_coefficients = []\n","\n","# Iterate through each fold\n","fold = 1\n","for train_index, test_index in kf.split(X_scaled):\n","    print(f\"\\nFold {fold}\")\n","\n","    # Split the data into training and testing sets\n","    X_train, X_test = X_scaled.iloc[train_index], X_scaled.iloc[test_index]\n","    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n","\n","    # Convert to numpy arrays\n","    X_train_np = X_train.values\n","    X_test_np = X_test.values\n","    y_train_np = y_train.values.reshape(-1, 1)\n","    y_test_np = y_test.values.reshape(-1, 1)\n","\n","    # Add intercept term to X_train and X_test\n","    ones_train = np.ones((X_train_np.shape[0], 1))\n","    ones_test = np.ones((X_test_np.shape[0], 1))\n","\n","    X_train_b = np.hstack([ones_train, X_train_np])\n","    X_test_b = np.hstack([ones_test, X_test_np])\n","\n","    # Calculate beta coefficients using Normal Equation\n","    beta = np.linalg.inv(X_train_b.T.dot(X_train_b)).dot(X_train_b.T).dot(y_train_np)\n","\n","    # Make predictions on the test set\n","    y_pred = X_test_b.dot(beta)\n","\n","    # Calculate R2 score\n","    r2 = r2_score(y_test_np, y_pred)\n","\n","    # Append results\n","    r2_scores.append(r2)\n","    beta_coefficients.append(beta)\n","\n","    print(f\"R2 Score: {r2}\")\n","\n","    fold += 1\n","# Find the index of the best R2 score\n","best_index = np.argmax(r2_scores)\n","best_beta = beta_coefficients[best_index]\n","\n","print(f\"\\nBest R2 Score obtained in fold {best_index + 1}: {r2_scores[best_index]}\")\n","print(\"Best Beta Coefficients:\")\n","print(best_beta.flatten())\n","\n","# Split the entire dataset into 70% training and 30% testing\n","X_train_full, X_test_full, y_train_full, y_test_full = train_test_split(\n","    X_scaled, y, test_size=0.3, random_state=42)\n","\n","# Convert to numpy arrays\n","X_train_full_np = X_train_full.values\n","X_test_full_np = X_test_full.values\n","y_train_full_np = y_train_full.values.reshape(-1, 1)\n","y_test_full_np = y_test_full.values.reshape(-1, 1)\n","\n","# Add intercept term\n","ones_train_full = np.ones((X_train_full_np.shape[0], 1))\n","ones_test_full = np.ones((X_test_full_np.shape[0], 1))\n","\n","X_train_full_b = np.hstack([ones_train_full, X_train_full_np])\n","X_test_full_b = np.hstack([ones_test_full, X_test_full_np])\n","\n","# Make predictions on the test set using best beta\n","y_pred_full = X_test_full_b.dot(best_beta)\n","\n","# Calculate R2 score\n","r2_full = r2_score(y_test_full_np, y_pred_full)\n","\n","print(f\"\\nR2 Score on 30% test data using best beta: {r2_full}\")\n","\n","plt.figure(figsize=(10,6))\n","plt.scatter(y_test_full, y_pred_full, alpha=0.7, color='b')\n","plt.plot([y_test_full.min(), y_test_full.max()], [y_test_full.min(), y_test_full.max()], 'r--')\n","plt.xlabel('Actual Prices')\n","plt.ylabel('Predicted Prices')\n","plt.title('Actual vs Predicted Prices')\n","plt.show()\n"],"metadata":{"id":"g5Rf2WJc64Gy"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Ques 2"],"metadata":{"id":"xUjxTN_aBB-v"}},{"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import r2_score\n","from sklearn.preprocessing import StandardScaler\n","\n","# Load the dataset\n","df = pd.read_csv('USA_Housing (1).csv')\n","\n","# Split the dataset into input features (X) and output variable (y)\n","X = df.drop('Price', axis=1)\n","y = df['Price'].values.reshape(-1, 1)\n","\n","# Step 1: Scale the input features\n","scaler = StandardScaler()\n","X_scaled = scaler.fit_transform(X)\n","\n","# Step 2: Add a column of ones to X_scaled (for the intercept term)\n","X_scaled = np.insert(X_scaled, 0, 1, axis=1)  # Adds the intercept (bias term)\n","\n","# Step 3: Split dataset into train (56%), validation (14%), and test (30%)\n","X_train, X_temp, y_train, y_temp = train_test_split(X_scaled, y, test_size=0.44, random_state=42)\n","X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.68, random_state=42)\n","\n","# Step 4: Gradient Descent function\n","def gradient_descent(X, y, alpha, num_iterations):\n","    m, n = X.shape\n","    beta = np.zeros((n, 1))  # Initialize regression coefficients (beta) to zeros\n","    cost_history = []\n","\n","    for i in range(num_iterations):\n","        predictions = X.dot(beta)\n","        error = predictions - y\n","        cost = (1/(2*m)) * np.sum(error ** 2)\n","        cost_history.append(cost)\n","        gradient = (1/m) * X.T.dot(error)  # Gradient\n","        beta = beta - alpha * gradient  # Update beta\n","\n","    return beta, cost_history\n","\n","# Step 5: Training the model for different learning rates\n","learning_rates = [0.001, 0.01, 0.1, 1]\n","num_iterations = 1000\n","best_r2_val = -np.inf\n","best_beta = None\n","best_lr = None\n","\n","for lr in learning_rates:\n","    print(f\"Training with learning rate: {lr}\")\n","    beta, cost_history = gradient_descent(X_train, y_train, alpha=lr, num_iterations=num_iterations)\n","\n","    # Step 6: Compute R² score on validation set\n","    val_predictions = X_val.dot(beta)\n","    r2_val = r2_score(y_val, val_predictions)\n","\n","    # Print the R² score for validation set\n","    print(f\"R² score on validation set: {r2_val}\")\n","\n","    if r2_val > best_r2_val:\n","        best_r2_val = r2_val\n","        best_beta = beta\n","        best_lr = lr\n","\n","# Step 7: Final evaluation on test set using the best beta\n","test_predictions = X_test.dot(best_beta)\n","final_r2_test = r2_score(y_test, test_predictions)\n","\n","print(f\"\\nBest learning rate: {best_lr}\")\n","print(f\"Best R² score on validation set: {best_r2_val}\")\n","print(f\"Final R² score on test set: {final_r2_test}\")\n","\n","# Optional: Plot cost history to visualize convergence\n","#import matplotlib.pyplot as plt\n","\n","#plt.plot(cost_history)\n","#plt.title(f\"Cost Reduction over Iterations (Learning Rate = {best_lr})\")\n","#plt.xlabel(\"Iterations\")\n","#plt.ylabel(\"Cost (Mean Squared Error)\")\n","#plt.show()\n"],"metadata":{"id":"P-jg0k39A_uu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Ques 3"],"metadata":{"id":"FqXJ_KjSD6d2"}},{"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","\n","colnames= [\"symboling\", \"normalized_losses\", \"make\", \"fuel_type\", \"aspiration\",\"num_doors\", \"body_style\", \"drive_wheels\",\n","            \"engine_location\", \"wheel_base\", \"length\", \"width\", \"height\", \"curb_weight\",\n","            \"engine_type\", \"num_cylinders\", \"engine_size\", \"fuel_system\", \"bore\", \"stroke\",\n","            \"compression_ratio\", \"horsepower\", \"peak_rpm\", \"city_mpg\", \"highway_mpg\", \"price\"]\n","df=pd.read_csv('imports-85.data.txt',header=None, names=colnames, na_values=\"?\" )\n","\n","df=df.replace('?',np.NaN)\n","df.head()\n","\n","from sklearn.impute import SimpleImputer\n","df.dropna(subset=['price'],inplace=True)\n","imputer = SimpleImputer(missing_values = np.nan,strategy ='most_frequent') #replace NAN with mode\n","imputer1=imputer.fit(df)\n","df=pd.DataFrame(imputer1.transform(df)) #pd.DataFrame is required becaude by default imputer returns an ndarray\n","df.columns=colnames #specifying column names\n","print(df)\n","\n","dict1={\"one\":1,\"two\":2,\"three\":3,\"four\":4,\"five\":5,\"six\":6,\"seven\":7,\"eight\":8,\"nine\":9,\"zero\":0,\"twelve\":12} #mapping the values\n","df['num_doors']=df['num_doors'].map(dict1)\n","df['num_cylinders']=df['num_cylinders'].map(dict1)\n","print(df)\n","\n","#dummy encoding scheme\n","dummy_encoding_body_style=pd.get_dummies(df['body_style'],prefix=\"body_style\",drop_first=True)\n","df=pd.concat([df,dummy_encoding_body_style],axis=1)\n","df.drop(['body_style'],axis=1,inplace=True)\n","dummy_encoding_drive_wheels=pd.get_dummies(df['drive_wheels'],prefix=\"drive_wheels\",drop_first=True)\n","df.drop(['drive_wheels'],axis=1,inplace=True)\n","df=pd.concat([df,dummy_encoding_drive_wheels],axis=1)\n","print(df)\n","\n","#label encoding\n","from sklearn.preprocessing import LabelEncoder\n","encoder=LabelEncoder()\n","df['make']=encoder.fit_transform(df['make'])\n","df['aspiration']=encoder.fit_transform(df['aspiration'])\n","df['engine_location']=encoder.fit_transform(df['engine_location'])\n","df['fuel_type']=encoder.fit_transform(df['fuel_type'])\n","df.head()\n","\n","cond=(df['fuel_system'].str.contains('pfi')==True)\n","df.loc[cond,'fuel_system']=1\n","cond2=(df['fuel_system']).str.contains('pfi')==False\n","df.loc[cond2,'fuel_system']=0\n","\n","cond3=(df['engine_type'].str.contains('ohc')==True)\n","cond4=((df['engine_type']).str.contains('ohc')==False)\n","df.loc[cond3,'engine_type']=1\n","df.loc[cond4,'engine_type']=0\n","\n","df.head()\n","df.info()\n","\n","x_data=df.loc[:,df.columns!='price']\n","y_data=df.loc[:,df.columns=='price']\n","\n","x_data.info()\n","y_data.info()\n","\n","#Scaling the input data\n","from sklearn.preprocessing import MinMaxScaler\n","scaler=MinMaxScaler()\n","features_to_scale=x_data.select_dtypes(np.number).columns\n","# print(df.columns)\n","x_scaled=scaler.fit_transform(x_data[features_to_scale]) #specifying the columns to normalise\n","x_Scaled=pd.DataFrame(x_scaled,columns=features_to_scale)\n","x_data=x_Scaled #assigning the scaled values to x_data\n","print(x_data)\n","\n","#Train a linear regressor on 70% of data (using inbuilt linear regression function of Python) and test its performance on remaining 30% of data.\n","from sklearn.linear_model import LinearRegression\n","from sklearn.model_selection import train_test_split\n","x_train,x_test,y_train,y_test=train_test_split(x_data,y_data,test_size=0.3,random_state=42)\n","lr=LinearRegression()\n","lr.fit(x_train,y_train)\n","print(lr.score(x_test,y_test))\n","\n","#Dimensional Reduction using PCA\n","# Check the number of features after processing\n","print(f\"Number of features after preprocessing: {x_data.shape[1]}\")\n","\n","# Determine the maximum number of components for PCA\n","max_components = x_data.shape[1]  # This should give you the number of features\n","print(f\"Maximum components for PCA: {max_components}\")\n","\n","# Dimensional Reduction using PCA\n","from sklearn.decomposition import PCA\n","from sklearn.linear_model import LinearRegression\n","\n","# Set n_components to the minimum of 6 or max_components\n","n_components = min(max_components, 6)  # Adjust to the number of features available\n","print(f\"Using {n_components} components for PCA\")\n","\n","pca = PCA(n_components=n_components)\n","x_pca = pca.fit_transform(x_data)\n","\n","# Split the PCA transformed data into training and testing sets\n","x_train_pca, x_test_pca, y_train_pca, y_test_pca = train_test_split(x_pca, y_data, test_size=0.3, random_state=42)\n","\n","# Train a linear regressor on PCA transformed data\n","lr_pca = LinearRegression()\n","lr_pca.fit(x_train_pca, y_train_pca)\n","\n","# Evaluate the model\n","pca_score = lr_pca.score(x_test_pca, y_test_pca)\n","print(f\"R² Score after PCA: {pca_score:.4f}\")\n","\n"],"metadata":{"id":"jVmkKUPjFX-V"},"execution_count":null,"outputs":[]}]}