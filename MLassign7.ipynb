{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Ques 1:"
      ],
      "metadata": {
        "id": "EF1wwPuYMVeJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1HX7UZDuY6wK"
      },
      "outputs": [],
      "source": [
        "# Q1\n",
        "import pandas as pd\n",
        "data_path = '/content/USA_Housing.csv'\n",
        "data = pd.read_csv(data_path)\n",
        "data.head()\n",
        "\n",
        "data_omit_last = data.iloc[:, :-1]\n",
        "data_omit_last.head()\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "data_scaled = scaler.fit_transform(data_omit_last)\n",
        "\n",
        "from sklearn.cluster import KMeans\n",
        "import matplotlib.pyplot as plt\n",
        "k = 3\n",
        "kmeans = KMeans(n_clusters=k, random_state=42)\n",
        "kmeans.fit(data_scaled)\n",
        "labels = kmeans.labels_\n",
        "\n",
        "inertia = []\n",
        "k_range = range(1, 11)\n",
        "\n",
        "for k in k_range:\n",
        "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
        "    kmeans.fit(data_scaled)\n",
        "    inertia.append(kmeans.inertia_)\n",
        "\n",
        "plt.figure(figsize=(8, 4))\n",
        "plt.plot(k_range, inertia, marker='o')\n",
        "plt.xlabel('Number of Clusters (k)')\n",
        "plt.ylabel('Inertia')\n",
        "plt.title('Elbow Method for Optimal k')\n",
        "plt.show()\n",
        "\n",
        "k_optimal = 3\n",
        "kmeans = KMeans(n_clusters=k_optimal, random_state=42)\n",
        "kmeans.fit(data_scaled)\n",
        "\n",
        "final_labels = kmeans.labels_\n",
        "data_omit_last['Cluster'] = final_labels\n",
        "data_omit_last.head()\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "pca = PCA(n_components=2)\n",
        "data_pca = pca.fit_transform(data_scaled)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(data_pca[:, 0], data_pca[:, 1], c=final_labels, cmap='viridis', marker='o', edgecolor='k')\n",
        "plt.xlabel('PCA Component 1')\n",
        "plt.ylabel('PCA Component 2')\n",
        "plt.title('K-Means Clusters (PCA-Reduced)')\n",
        "plt.colorbar(label='Cluster')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques 2:"
      ],
      "metadata": {
        "id": "Ua9nkOoOMTYS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Q2\n",
        "!pip install scikit-learn-extra\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import silhouette_score\n",
        "from sklearn_extra.cluster import KMedoids\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "data_path = '/content/USA_Housing.csv'\n",
        "data = pd.read_csv(data_path)\n",
        "data_omit_last = data.iloc[:, :-1]\n",
        "\n",
        "scaler = StandardScaler()\n",
        "data_scaled = scaler.fit_transform(data_omit_last)\n",
        "sse = []\n",
        "k_range = range(1, 11)\n",
        "\n",
        "for k in k_range:\n",
        "    kmedoids = KMedoids(n_clusters=k, random_state=42, method='pam')\n",
        "    kmedoids.fit(data_scaled)\n",
        "    sse.append(kmedoids.inertia_)\n",
        "\n",
        "plt.figure(figsize=(8, 4))\n",
        "plt.plot(k_range, sse, marker='o')\n",
        "plt.xlabel('Number of Clusters (k)')\n",
        "plt.ylabel('SSE (Inertia)')\n",
        "plt.title('Elbow Method for Optimal k (K-Medoids)')\n",
        "plt.show()\n",
        "\n",
        "k_optimal = 3\n",
        "kmedoids = KMedoids(n_clusters=k_optimal, random_state=42, method='pam')\n",
        "kmedoids.fit(data_scaled)\n",
        "\n",
        "labels = kmedoids.labels_\n",
        "data_omit_last['Cluster'] = labels\n",
        "silhouette_avg = silhouette_score(data_scaled, labels)\n",
        "print(f'Silhouette Score for k={k_optimal}: {silhouette_avg:.2f}')\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "pca = PCA(n_components=2)\n",
        "data_pca = pca.fit_transform(data_scaled)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(data_pca[:, 0], data_pca[:, 1], c=labels, cmap='viridis', marker='o', edgecolor='k')\n",
        "plt.xlabel('PCA Component 1')\n",
        "plt.ylabel('PCA Component 2')\n",
        "plt.title('K-Medoids Clusters (PCA-Reduced)')\n",
        "plt.colorbar(label='Cluster')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "D77TcMp9MSaJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "S(i)=\n",
        "max(a(i),b(i))/\n",
        "b(i)−a(i)\n",
        "​\n",
        "\n",
        "\n",
        "The Silhouette Score is an important metric used in clustering to evaluate the quality of the clusters produced by a clustering algorithm like K-Means. In particular, it helps assess how well-defined and well-separated the clusters are. Using the silhouette score alongside the Elbow Method can give you a more robust way to select the optimal number of clusters (k).\n",
        "\n",
        "The **Silhouette Score** is not a direct alternative to the **Elbow Method**; rather, it is a complementary evaluation metric that can be used **along with** the Elbow Method to better assess the quality of the clusters formed by a clustering algorithm like **K-Means**.\n",
        "\n",
        "Here’s why using the **Silhouette Score** alongside the **Elbow Method** is a good idea:\n",
        "\n",
        "### 1. **Elbow Method (Inertia)**\n",
        "The **Elbow Method** helps identify the **range** of values for `k` (the number of clusters) where the inertia (within-cluster sum of squared distances) starts to level off.\n",
        "- **Pros**: It’s a simple and widely used heuristic that helps you visually find the point where increasing `k` no longer results in significant improvements in clustering.\n",
        "- **Limitations**: It only measures how tightly the points are packed within clusters but **doesn't account for how well-separated the clusters are**. This means that clusters could be tight but overlap with each other, making them poorly defined.\n",
        "\n",
        "### 2. **Silhouette Score**\n",
        "The **Silhouette Score** measures the **quality** of clustering by considering both the cohesion (how close the points within the same cluster are) and separation (how far apart the clusters are). It helps you understand how well-defined and distinct your clusters are.\n",
        "- **Pros**: The Silhouette Score is a **direct measure of clustering quality**, and it takes both cohesion and separation into account. This gives you more information about the cluster structure, and it’s not just about minimizing inertia.\n",
        "- **Limitations**: It can be sensitive to the **shape** of the clusters, and if the data doesn't form clearly separated clusters (e.g., non-convex clusters), the silhouette score may not perform well.\n",
        "\n",
        "### Using **Both Methods Together**\n",
        "By using the **Elbow Method** and **Silhouette Score** together, you get a more robust understanding of how to choose the optimal number of clusters:\n",
        "\n",
        "1. **Step 1: Use the Elbow Method** to identify a **range** of possible values for `k`. This gives you an idea of where inertia starts to level off.\n",
        "   - You can **look for the \"elbow\"** in the inertia plot, which is often the point where the rate of decrease in inertia slows down.\n",
        "   \n",
        "2. **Step 2: Use the Silhouette Score** to **refine your choice of `k`** within that range.\n",
        "   - The silhouette score helps you confirm whether the clusters are well-defined and distinct at each candidate `k`. You may find that even if the inertia starts to level off at a certain `k`, the silhouette score suggests that a different `k` gives the best clustering quality.\n",
        "\n",
        "### **Why Use Both Methods?**\n",
        "- **Elbow Method** tells you where the inertia (error) stabilizes, but **Silhouette Score** tells you if those clusters are well-separated and meaningful.\n",
        "- In some cases, the elbow method might suggest a larger `k`, but the silhouette score could indicate that fewer clusters give a more optimal separation. For example:\n",
        "  - You might find that `k=6` gives a sharp elbow in the inertia plot, but the silhouette score is highest for `k=4`. In this case, `k=4` is likely the better choice.\n",
        "\n",
        "### Example Workflow:\n",
        "\n",
        "#### 1. **Elbow Method**:\n",
        "   - First, plot the inertia for different values of `k` (1 to 10, for example).\n",
        "   - Look for the **elbow** where the rate of inertia reduction slows down. This is typically a good starting point for selecting `k`.\n",
        "\n",
        "#### 2. **Silhouette Score**:\n",
        "   - After identifying the possible range of `k` from the elbow plot, compute the silhouette score for each value of `k` (2 to 10).\n",
        "   - Choose the `k` with the **highest silhouette score**, which gives you the **most well-defined clusters** in terms of both **cohesion** and **separation**.\n",
        "\n",
        "### Example Scenario:\n",
        "Let’s say you run both methods on a dataset, and the results are as follows:\n",
        "\n",
        "- **Elbow Method**: The elbow appears at `k=4`, suggesting that increasing `k` beyond 4 doesn’t significantly reduce inertia.\n",
        "- **Silhouette Scores**: The silhouette score for `k=4` is `0.72`, which is high, but the silhouette score for `k=5` is `0.68`, which is lower.\n",
        "\n",
        "In this case, even though the elbow method suggests `k=4` as the optimal point based on inertia, the silhouette score reinforces that `k=4` is indeed the best choice for the **quality** of the clusters.\n",
        "\n",
        "### When to Rely on One Over the Other?\n",
        "- **If the Elbow Method is unclear** (e.g., if there's no clear \"elbow\" or if the inertia decreases very gradually), the **Silhouette Score** can help you choose a more meaningful `k`.\n",
        "- **If the Elbow Method suggests a range of possible values for `k`**, the **Silhouette Score** can help you **narrow it down** to the best `k` that produces well-defined and separated clusters.\n",
        "\n",
        "### Conclusion:\n",
        "- **The Elbow Method** and **Silhouette Score** serve complementary purposes: one helps find a range of possible `k` values, while the other helps assess the quality of the clustering.\n",
        "- **Use both methods together** to make a more informed decision when selecting the optimal number of clusters.\n",
        "\n",
        "Would you like to dive deeper into the calculations or implementation details for using these methods together?\n"
      ],
      "metadata": {
        "id": "3GZodmNQeUMO"
      }
    }
  ]
}